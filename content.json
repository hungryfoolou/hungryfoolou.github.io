{"meta":{"title":"圈亮的博客","subtitle":"Don't be shy->Always try","description":"Day Day Up","author":"圈亮","url":"https://hungryfoolou.github.io"},"pages":[{"title":"","date":"2019-04-21T09:14:11.807Z","updated":"2019-04-21T09:14:00.840Z","comments":true,"path":"baidu_verify_DbO2NEiiDN.html","permalink":"https://hungryfoolou.github.io/baidu_verify_DbO2NEiiDN.html","excerpt":"","text":"DbO2NEiiDN"},{"title":"archives","date":"2018-08-16T07:07:45.000Z","updated":"2018-08-16T07:08:14.205Z","comments":true,"path":"archives/index.html","permalink":"https://hungryfoolou.github.io/archives/index.html","excerpt":"","text":""},{"title":"About me","date":"2018-08-16T07:15:10.000Z","updated":"2018-11-25T05:35:59.858Z","comments":true,"path":"about/index.html","permalink":"https://hungryfoolou.github.io/about/index.html","excerpt":"","text":"I am a Student from XIDIAN University.I graduated from GUIZHOU University.I’m so glad to share my stories with you.You can contact me with 2782680579@qq.com.You can also view my older blog at my CSDN’s blog."},{"title":"categories","date":"2018-08-16T07:16:51.000Z","updated":"2018-08-16T08:01:22.149Z","comments":true,"path":"categories/index.html","permalink":"https://hungryfoolou.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"如何使用循环修改python的字典中的数据","slug":"如何使用循环修改python的字典中的数据","date":"2019-04-25T13:45:45.000Z","updated":"2019-04-25T13:54:47.021Z","comments":true,"path":"2019/04/25/如何使用循环修改python的字典中的数据/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/25/如何使用循环修改python的字典中的数据/","excerpt":"","text":"Question Description不能通过如下for循环修改字典中的数据，因为修改后for循环需要遍历的数据就变化了，对于python的for循环这是不允许的。如果要实现的功能是：如果字典的键为’sb’，则把字典中的这个键值对去掉。比如不能像下面这样修改：123for tmp_i in dict: if dict[tmp_i]=='sb': del dict[tmp_i] Solution可以使用while循环来修改数据，但是需要先将字典的键变为list，使得方便迭代。1234567for tmp_i in dict:tmp_i = 0while tmp_i &lt; len(dict): if dict[dict[tmp_i]] == 'sb': del dict[dict[tmp_i]] tmp_i = tmp_i + 1","categories":[{"name":"Python","slug":"Python","permalink":"https://hungryfoolou.github.io/categories/Python/"}],"tags":[]},{"title":"文章标题可以超过50个字符了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了","slug":"文章标题可以超过50个字符了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了","date":"2019-04-21T05:20:19.000Z","updated":"2019-04-22T10:48:43.652Z","comments":true,"path":"2019/04/21/文章标题可以超过50个字符了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/21/文章标题可以超过50个字符了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了了/","excerpt":"","text":"标题可以超过50个字符了哈哈哈，之前起文章标题总是起英文名，总怕起中文名一不小心就超过50个字符了，后来发现英文的文章标题也容易超过50个字符，解决办法: Hexo NexT使用Gitalk未找到相关的Issues进行评论Error:Validation Failed，部分文章的评论区会报Error: Validation Failed,具体原因是由于 Github 限制 label 长度不能超过 50引起的，解决方法是通过MD5加密ID来缩短label长度。 放一个视频kinjaz超酷舞蹈","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"The first time to send danmaku in bilibili","slug":"The-first-time-to-send-danmaku-in-bilibili","date":"2019-04-20T10:03:47.000Z","updated":"2019-04-20T12:34:10.591Z","comments":true,"path":"2019/04/20/The-first-time-to-send-danmaku-in-bilibili/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/20/The-first-time-to-send-danmaku-in-bilibili/","excerpt":"","text":"买了明天的一张音乐会的票：《四月是你的谎言》–“公生”与“熏”的钢琴小提琴唯美经典音乐集，为了避免像上次去参加玩野潮青年音乐节听歌时不会唱很多歌的尴尬局面，刚在b站上把《四月是你的谎言》补了，至少熟悉一下背景。总的而言，这个番比天线宝宝好看很多，倒也没同学说的那么让人抑郁（再抑郁也比不上《人间失格》），感觉挺好的，熏在知道自己剩下的日子不多了，是想着如何利用好这些日子，要吃好吃的，要随意地不按照谱子拉小提琴，要和公生一起合奏，帮助公生走出了阴影。看完这个番后，想发个弹幕却发现等级不够，于是回答了蛮多题目才升了一级，发了一个弹幕。下面是这部番里面美美的图片：","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"Error 1405 comes when use pymysql in pycharm to connect mysql","slug":"Error-1405-comes-when-use-pymysql-in-pycharm-to-connect-mysql","date":"2019-04-18T09:45:45.000Z","updated":"2019-04-20T09:56:02.833Z","comments":true,"path":"2019/04/18/Error-1405-comes-when-use-pymysql-in-pycharm-to-connect-mysql/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/18/Error-1405-comes-when-use-pymysql-in-pycharm-to-connect-mysql/","excerpt":"","text":"目标在pycharm中连接mysql 问题描述使用pymysql模块连接mysql时报了错误：11045, &quot;Access denied for user &apos;root&apos;@&apos;DESKTOP-GESB8H7&apos; (using password: YES)&quot; 解决方法将连接语句的host值设置为locaohost而非自己电脑的ip地址。即如下设置：1conn = pymysql.connect(host='localhost',port= 3306,user = 'root',passwd='oglhao123',db='cnnvd') # db：数据库的库名 感想出现这个问题的原因我猜想是由于win10强大的安全机制，喜欢拒绝别人。","categories":[{"name":"database","slug":"database","permalink":"https://hungryfoolou.github.io/categories/database/"}],"tags":[]},{"title":"Help crawler avoid being banned","slug":"Help-crawler-avoid-being-banned","date":"2019-04-13T09:34:22.000Z","updated":"2019-04-20T09:43:50.058Z","comments":true,"path":"2019/04/13/Help-crawler-avoid-being-banned/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/13/Help-crawler-avoid-being-banned/","excerpt":"","text":"目标scrapy爬虫时避免被禁止 问题描述爬虫报了错误：123452019-04-13 13:56:47 [scrapy.core.scraper] ERROR: Error downloading &lt;GET https://www.exploit-db.com/exploits/8937&gt;Traceback (most recent call last): File &quot;F:\\App_apply\\Python\\Anaconda\\apply\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py&quot;, line 43, in process_request defer.returnValue((yield download_func(request=request,spider=spider)))twisted.web._newclient.ResponseNeverReceived: [&lt;twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.&gt;] 解决方法 修改settings.py，设置为 1DOWNLOAD_DELAY = 1.5 # 1-2之间比较合适 其他设置参考：博客 后来发现即使像第2步这样设置了还是会报这个问题描述中的错误，暂时没找到合适的解决办法，不过发现一个现象：爬虫的数据量比较少时没出现这个错误，数据量变大了就会出现这个错误。 感想数据一旦变得较大，就容易出现问题，问题少少益善。","categories":[{"name":"web crawler","slug":"web-crawler","permalink":"https://hungryfoolou.github.io/categories/web-crawler/"}],"tags":[]},{"title":"Import csv format of mysql's data into navicat","slug":"Import-csv-format-of-mysql-s-data-into-navicat","date":"2019-04-11T08:58:01.000Z","updated":"2019-04-20T09:14:29.818Z","comments":true,"path":"2019/04/11/Import-csv-format-of-mysql-s-data-into-navicat/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/11/Import-csv-format-of-mysql-s-data-into-navicat/","excerpt":"","text":"目标想把师兄从mysql导出的cnnvd数据导入navicat中。 问题描述目前有数据的xls格式，用navicat导入xls格式数据时navicat停止了工作 解决方法 先用excel打开xls格式的数据文件，另存为csv格式的数据文件； 用notepad++打开csv格式的文件，选择菜单的编码-&gt;转换为utf-8格式； 在navicat里新建数据库cnnvd，数据库名为cnnvd，字符集为utf8 -- UTF-8 Unicode，排序规则为utf8_general_ci； 在导入向导的第2步选择编码格式为utf-8格式； 在导入向导的第6步，把类型从默认的varchar改为longtext，因为有的属性的值的长度比较大。 感想 为了避免导入数据的格式错误，需保证将要导入的数据和建立的数据库的编码方式相同。 数据一旦变得较大，就容易出现问题，额，希望问题少些。 师兄也给了xml格式的数据文件，没有尝试导入，应该只要编码格式一致就可以导入。","categories":[{"name":"web crawler","slug":"web-crawler","permalink":"https://hungryfoolou.github.io/categories/web-crawler/"}],"tags":[]},{"title":"How to install scarpy","slug":"How-to-install-scarpy","date":"2019-04-10T08:43:22.000Z","updated":"2019-04-20T09:07:38.049Z","comments":true,"path":"2019/04/10/How-to-install-scarpy/","link":"","permalink":"https://hungryfoolou.github.io/2019/04/10/How-to-install-scarpy/","excerpt":"","text":"目标在win10上安装scrapy。 问题描述打算在win10上安装scrapy框架，因为anaconda安装模块会自动解决模块间的依赖问题，本来以为使用命令conda install scrapy会省一些麻烦，结果居然报了挺多错误。 解决方法用pip安装，用命令pip install scrapy即可安装scrapy。 感想简单的操作方法不适用于所有情况，可能反而变得复杂。","categories":[{"name":"web crawler","slug":"web-crawler","permalink":"https://hungryfoolou.github.io/categories/web-crawler/"}],"tags":[]},{"title":"why file name of windows os can't include some characters","slug":"why-file-name-of-windows-os-can-t-include-some-characters","date":"2019-03-09T08:40:20.000Z","updated":"2019-03-09T09:36:21.232Z","comments":true,"path":"2019/03/09/why-file-name-of-windows-os-can-t-include-some-characters/","link":"","permalink":"https://hungryfoolou.github.io/2019/03/09/why-file-name-of-windows-os-can-t-include-some-characters/","excerpt":"","text":"windows7、windows10操作系统的文件夹命名不能包括？：等符号，详细说明见:https://docs.microsoft.com/zh-cn/previous-versions/s6feh8zw(v=vs.110) （完整链接包括最后vs.110右边的括号） 理由如下：有很多字符在shell中有特殊含义，用在文件名中就会导致错误的解析。除了保留字符外，还有保留字：CON NUL COM1 COM2 LPT1 PRN。例如：move a.txt NUL就相当于 del a.txt 保留字符： \\：路径分割符 /： :：盘符 ?：通配符 ‘ “：字符串 &lt;&gt; | &amp;：重定向符 ()： {}： %：变量 参考：豆瓣","categories":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"https://hungryfoolou.github.io/categories/basic-knowledge/"}],"tags":[]},{"title":"Mom's birthday of 2019","slug":"Mom-s-birthday-of-2019","date":"2019-02-28T01:49:39.000Z","updated":"2019-04-20T12:06:07.960Z","comments":true,"path":"2019/02/28/Mom-s-birthday-of-2019/","link":"","permalink":"https://hungryfoolou.github.io/2019/02/28/Mom-s-birthday-of-2019/","excerpt":"","text":"妈妈生日快乐^_^昨天是妈妈的生日，客厅的气球等饰品美丽地凸显出了生日主题。昨日过生的除了还有两个亲戚外(一个是胡姑爷，一个是唐敏的表姐/妹李清清)，还有本科同学冯叶，她说她已知的还有四五个也是在昨日过生。昨天可真是个幸福的日子。上图（高清照片，加载较慢）：1. 2. 3. 4. 5. 6. 7.","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"new year of 2019","slug":"new-year-of-2019","date":"2019-02-19T02:26:54.000Z","updated":"2019-02-19T04:56:56.751Z","comments":true,"path":"2019/02/19/new-year-of-2019/","link":"","permalink":"https://hungryfoolou.github.io/2019/02/19/new-year-of-2019/","excerpt":"","text":"随手胡写放假时间：2019.1.29-2019.2.181月29号从西安出发，30号中午到了重庆，晚上去看了外甥女丫丫，姐夫的妈妈回了西安，姐姐要上班，所以姐姐吩咐我和姐夫照顾丫丫。大概照顾了3天丫丫。每天8点之前从熙街走到富力城，大概9点左右丫丫醒来，帮忙穿衣、戴帽子、穿鞋、换尿布湿，紧接着就是喝水、喂奶、吃火龙果/香蕉等，然后逗娃。逗娃基本上就是把自己当成一傻子，越傻越好。娃不开心了，就把娃抱起来或者喂她吃的。有时也看看小猪佩奇、天线宝宝、宝宝巴士，可以省些带娃的体力，不过看了半个小时就得休息。基本就是吃了睡，睡了吃。在冰箱上贴了一个记录表，记录着娃每天的身高、体重、吃的奶的次数、便便的颜色等等。姐姐下午下班，晚上吃了晚饭，我就回熙街去爸妈那儿了。2月3号上午去璧山的奥特莱斯买衣服，我买了一双阿迪的小白鞋，可惜店里没有卖经典的三叶草。之前给丫丫买的鹿子气球本来在妈妈的包包上拴得好好的，不知怎的在奥特莱斯买衣服的时候鹿子走丢了。下午去枫香湖儿童公园玩了玩，走累了就回家了。2月4号（农历12月30号）上午我和爸妈打嘀嗒顺风车回铜梁，由于老家雍溪家里大人都基本外出了，所以我们坐客车回万古去吃小姨的席，小姨在万古的房子被拆了用于建新楼盘，分到了小区里的面积与旧房相近的新房，因而设宴席庆祝，见到了表姐唐敏，弄得金色的头发挺好看。万古小学的名字变成了大足区第三小学，没有去看母校万古镇中，初中时读的万古镇中不知道变成了什么样子。席后，下午去小姨新家里坐了会，然后去三姨、胡姑爷的老家里去玩了会，我没想到两地如此之近，坐轿车花了未到10分钟。三姨的呼啦圈变大了，胡姑爷变瘦了，胡强变壮了，胡洋还是那么开朗。见到了唐露，个子高高的，涨了些肉，没以前那么看起来单薄了，弄的发型很时髦。见到了胡洋的伯妈的女儿，忘记名字了，上一次见面我还在上小学，这次见面我还以为她是农村的姑娘，因为她留着两个马尾辫，像极了村花。比我小一级，在西华大学念书，和胡洋读的一样的专业，机械。天色渐晚，准备回小姨家住一晚，初一早上再回。晚上老家打电话说时间紧要算账，妈妈这里有着账的明细，所以只得赶回雍溪老家。赶到家时，家里在靠烧烤，烤具是汉哥二爸他们从重庆带回来的，他们开着新车比亚迪唐回来，车相当拉风。吃了两串烤肉，聊了会天就准备回卧室休息了，因为往常今晚他们是要熬夜打牌的，我不大喜欢打牌，所以就早早休息了。2月5号（初一）早上吃了汤圆就去上坟了，上坟的途中还遇到蛮多亲戚，下午做了什么已经忘记。初二就回大学城熙街了。2月7号（初三）下午和姐姐他们在大学城公园和四川美术学院里面溜达，刚准备回去爸爸就接到老家的电话，说爷爷身体不好，拉血，应该是痔疮犯了，叫我们赶紧回家。然而已是下午四点多，不好打车回去，只得第二天回去，第二天我和爸妈回了家。然而见爷爷精神较好，也没拉血了，觉得是爷爷老病犯了，吃吃药过两天就好了，所以2月9号（初五）我们又回大学城了，以为爷爷的病缓和了，我们心里也缓和了写，晚上我和爸妈去看了《流浪地球》。2月11号（初七）凌晨3点多接到家里电话，说爷爷快要去世了。二爸从南坪过来接我、爸、姐（妈妈交了馆子的货再回老家）一起回了老家，没赶得上见到爷爷最后一面。真的没想到，前几天爷爷看起来精神挺好的，早知道初五就不回来了，还能见到最后一面。车开到离老家还有几十米的时候，二爸按响了喇叭，老家人在路上放了火炮。我们陆续走进了爷爷躺着的那个屋子，先是烧纸，然后磕头。后面来的人越来越多，好多事情我也记不清了。一有敲锣送花圈的来，我们就得跪着迎接，大概做得最多的事情就是烧纸和跪。老家的丧事很是讲究，要请道士来帮忙，道士弄了很多我们看不懂的事情。比如在院子里放一截香蕉的树干。后面爷爷的墓地地址选好了，就挨着奶奶的墓地，后面需要守三天墓地，直到农历十三（新历17号）早上下葬。我和哥哥们守了一天，后面两天因为爸妈担心我感冒加重就不让我去守了。十三爷爷下葬之后，我和姐、姐夫、妈妈就回大学城了，爸爸还需要在老家弄好一些后续的事情。2月18号坐了上午的高铁，下午到了学校，晚上没去实验室，只想感冒早点好，把新学期计划好。 在老家的期间，大概在初一初二的时候，老家人说要给我相亲，对象是村里的，还是小学同学。其实我是有些拒绝的，但是自己竟然答应了，不过过了几天得知对方已经有男朋友了，相亲就此打住了。 我也不知道自己写的啥，反正就想写点东西，随手胡写。","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"Keng of Xpath","slug":"Keng-of-Xpath","date":"2019-01-25T04:48:42.000Z","updated":"2019-01-25T05:28:34.689Z","comments":true,"path":"2019/01/25/Keng-of-Xpath/","link":"","permalink":"https://hungryfoolou.github.io/2019/01/25/Keng-of-Xpath/","excerpt":"","text":"坑 of Xpath昨天终于把折磨我很久的一个关于xpath的bug修复了。在此把坑说明一下。chrome浏览器按F12之后对源码右击选择Copy-&gt;Xpath即可获取某部分的Xpath路径（详见我的上一篇文章）。然而F12得到的源码并非真正的源码，真正的源码与F12得到的源码不一定相同。真正的源码可通过右击网页-&gt;查看网页源代码获取。举个例子，要爬取某个nvd页面的标题“CVE-2006-4206 Detail”，通过F12得到的XPath为1//*[@id=&quot;p_lt_WebPartZone1_zoneCenter_pageplaceholder_p_lt_WebPartZone1_zoneCenter_VulnerabilityDetail_VulnFormView&quot;]/tbody/tr/td/h2 而查看网页源代码发现真正的XPath为1//*[@id=&quot;p_lt_WebPartZone1_zoneCenter_pageplaceholder_p_lt_WebPartZone1_zoneCenter_VulnerabilityDetail_VulnFormView&quot;]/tr/td/h2 该xpath值比上一个的xpath值少了’tbody’ 坑的规律包含tbody的xpath通常会解析错误，即F12得到的xpath包含’tbody’而实际xpath并不包含’tbody’，就像上个例子，解决方法：把’tbody’去掉即可。 参考文章：scrapy框架中利用xpath获取网页内容为空，而xpath书写完全正确","categories":[{"name":"web crawler","slug":"web-crawler","permalink":"https://hungryfoolou.github.io/categories/web-crawler/"}],"tags":[]},{"title":"A easy way to get XPath","slug":"A-easy-way-to-get-XPath","date":"2018-12-18T08:00:59.000Z","updated":"2019-01-25T05:26:01.807Z","comments":true,"path":"2018/12/18/A-easy-way-to-get-XPath/","link":"","permalink":"https://hungryfoolou.github.io/2018/12/18/A-easy-way-to-get-XPath/","excerpt":"","text":"chrome获取Xpath主要有三种方式实现爬虫：正则表达式、BeautifulSoup和lxml。lxml可以由XPath实现，BeautifulSoup和lxml均可以由CSS选择器实现。网上对这三者的性能进行了对比，结果表示lxml最优。所以打算在项目中用lxml来爬虫，个人觉得XPath比CSS选择器好用，于是学习了一下XPath语法，今日却偶然发现了Chrome浏览器可以直接对某个需要的网页内容直接找到对应的XPath。以exploitdb为例，用chrome浏览器访问该链接，按F12后，按Ctrl+Shift+C或者点击控制台中的指针图标（select an element in the page to inspect it），点击欲爬取的网页区域之后，在控制台Elements下会在对应的html代码部分闪烁，右击该部分代码，选择Copy-&gt;Copy XPath. 获取某个标签下的所有文本用string方法，比如content=tree.xpath(&#39;string(xpath表达式)&#39;)即可获取xpath表达式标签下的所有文本 注意：为了避免xpath表达式有双引号””导致解析出错的情况，可以用”””代替string外边的’，即content=tree.xpath(&quot;&quot;&quot;string(xpath表达式)&quot;&quot;)","categories":[{"name":"web crawler","slug":"web-crawler","permalink":"https://hungryfoolou.github.io/categories/web-crawler/"}],"tags":[]},{"title":"basic konwledge of Deep Learning","slug":"basic-konwledge-of-Deep-Learning","date":"2018-12-06T03:09:38.000Z","updated":"2018-12-17T01:50:01.185Z","comments":true,"path":"2018/12/06/basic-konwledge-of-Deep-Learning/","link":"","permalink":"https://hungryfoolou.github.io/2018/12/06/basic-konwledge-of-Deep-Learning/","excerpt":"","text":"最近做英语听说的presentation，topic是modern sicence and new technology，所以想科普一下深度学习的基本知识。看了下《一天弄懂深度学习》(访问链接请科学上网)，对其中的three steps for deep leanring的第一章说一下自己的理解。 图一： 对于图一：第一步是bulid network structure，由于不同神经元的连接方式会产生不同的神经网络结构，所以首先确定神经元之间的连接方式。每个神经元可以看作是一个小的函数f，这个小函数有一个参数Wi（实际有很多参数，这里假设只有一个参数）,而所有神经元构成的神经网络可以看作是一个大函数F，这个大函数的参数为W1、W2……Wi……Wn和X，X是神经网络的外部输入参数（比如一幅图像），所有Wi参数均为神经网络本身的参数（我的通俗理解方法）。这个大函数可以看成是函数集合，因为不同的参数会产生不同的函数，这就对应第一步的build a set of function，然后我们需要确认这个大函数的所有Wi，确认的根据是什么呢？这就对应第二步define the goodness of a function，确认的根据就是Loss（损失函数，损失即真实结果与预测结果的差距)，第二步只是定义好了Loss，第三步就是pick the best function，根据损失函数不断迭代，最后得到最好的所有Wi，由此确认了大函数F。 图二： 对于图二：今天刚做完英语听说的presentation，讲完后问童鞋们有没有问题，然后老师就带头问了一下AI、ML、DL的区别，我当时演讲时就是用的这页slide的备注描述的。当时冯老师让我用英语再解释一下，当时脑壳卡壳了，用英语解释了半天也没解释清楚，后面老师让我用中文解释。这里大概复述一下：ML、DL都是为了实现AI的技术或者说方法，ML是实现AI的一种主流方式，ML包括很多算法，其中包括神经网络算法，而DL可以看作神经网络的衍生，DL可以看作是各种神经网络的组合。（当时为了方便童鞋老师的理解，说把DL看作是一种ML的算法，其实严格说来是很牵强的）","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://hungryfoolou.github.io/categories/Deep-Learning/"}],"tags":[]},{"title":"Network Security College Forum","slug":"Network-Security-College-Forum","date":"2018-11-24T13:51:24.000Z","updated":"2018-12-17T01:53:01.665Z","comments":true,"path":"2018/11/24/Network-Security-College-Forum/","link":"","permalink":"https://hungryfoolou.github.io/2018/11/24/Network-Security-College-Forum/","excerpt":"","text":"（由于中文文章标题URL过长导致Gitalk的Validation Failed，所以用的英文标题，尴尬以后应该能解决这个问题） 2018/11/23-24日参加了西电主办的一流网络安全学院建设发展论坛。论坛的各个大会报告大都与安全相关，上周三在北校区上课时看到了该论坛的宣传栏，某个大会报告题目十分吸引眼球：加上有一位本科的老师也有一个报告，所以立即就报名参加了。 日程安排1.23日日程安排2.24日日程安排(1)(2) 23日开幕式及报告1.早上6:30起床，7:10在西电南校区乘坐学校安排的大巴，到了陕西宾馆已是8点左右，我们到的挺早的，不过大会工作人员叫我们坐在第6排之后，所以我坐在了这个位置： 发现这些领导人致辞时的第一句话，要先说“尊敬的方滨兴院士、尊敬的xx牛逼人物”之后，才说我们这些普通群众。 2.接下来听了防火墙之父方院士的演讲，抛开其他因素单从演讲内容看，演讲是非常精彩的，浅显易懂，举了很多例子，比如人工智能、系统安全等，都能适用于他说的那个框架：（坐在后面，尴尬把方院士拍的有些模糊） 3.由于某些原因不能在陕西宾馆吃饭，中午就坐大巴回南校区了，由于实验室突然布置了一个作业，而且要下午交，十分犹豫要不要做，最后还是妥协下午不听李飞飞大佬的讲座了，明天再来听其他大佬的讲座。 24日报告上午上午由于有3个论坛同时进行，而我对《机器学习的安全问题》这个主题以及汪定（本科郭春老师提及好几次他的名字，说他各种厉害）比较感兴趣，所以选择了听密码学前沿论坛。有童鞋参加了其他两个论坛，因此得以补充内容，感谢。 密码学前沿论坛1.《实用量子保密查询协议研究进展》 高飞（北京邮电大学）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13) 2.《可修改数字签名》 黄欣沂（福建师范大学）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14) 3.《机器学习中的安全问题》（演讲时修改了题目） 李进（广州大学）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)攻击模型中，黑盒攻击比白盒攻击难度更高(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33)(34)(35) 笔记：(i)实验室有师兄师姐曾做过该方面研究，该老师的展示内容和实验室的资料有些相似之处，很有学习价值。(ii)大部分ML Security的论文大都是搞ML的人做的，从ML角度出发，可考虑从密码角度出发，关注ML安全的理论层次。 4.《Couuntering Cryptographic Subversions in the Post-Snowden Era》 陈荣茂（国防科技大学）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)标题是根据上一页ppt的：如果你选择停止追狗的尾部，你选择至少信任编译器时，会出现问题：编译器可能编译另一段代码（可能会导致危害的代码），而非本来想编译的代码。(15)(16)(17)(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28) 5.《口令中Zipf定律的启示》 汪定（北京大学）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30)(31)(32)(33) 6.《基于不对称信息的软件代码保护》 陈凯（中国科学院信息工程研究所）(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17)(18)(19)(20)(21)(22)(23) 笔记：对于语句 if(a==3),3即为key。 网络安全前沿论坛1.区块链及其安全问题初探(1)block chaim：分布式架构 (2)分类： (i)共有链，如比特币，超级账本 (ii)联盟链（最有发展前景） (iii)私有链 (3)应用场景：金融、能源、保险、交通、农产品溯源、医疗、电子商务（底层应用区块链，保证底层真实性，安全性）、慈善、数字娱乐、物流、政务等 (4)应用：智能合约项目，如计划通过物联网+区块链技术，提供智能锁等设备，把人们租赁关系用去中化的方式建立起来，如租赁自行车等（基于eos，以太坊等进行开发应用） (5)安全策略：传统算法不能适应区块链。 可以考虑量子密码算法、量子随机数发生器、设计新型的区块链体系结构（有向无环图、多图共识、密码抽签）、选择可靠的数据交易所。。。等 (6)智慧城市应用（通过ai+区块链+云+大数据技术实现智慧城市之间的互联） 2.社交数据处理与行为分析(1)社交网络与社交大数据实现社会网络：相对稳定的结构、节点通常为人、连接为社会关系在线社会网络 (2)社群发现：根据用户间交互关系：基于连接紧密度的社群发现根据用户间偏好关系：基于主题相似性的社群发现根据用户间移动关系：基于签到轨迹相似度的社群发现单模数据向多模演化（文本———文本、视频、音频等）线上—物理空间—现实社交 (3)单网络等社群发现(i)基于lda模型（参数推到-吉布斯采样得到社群、类型偏好、抵御偏好）(ii)多维特征映射与矩阵分解通过用户特征分解得到隐含特征 (4)跨社交网络等社交发现(i)基于重叠用户的跨社交网络社群发现(ii)面向非重叠用户的跨社交网络社群发现 (5)用户位置预测（下一个位置预测、任意时刻位置预测）【通过用户签到、评论等行为所携带的位置数据，挖掘用户生活模式等，进行建模】(i)基于多维特征融合的细粒度用户签到位置预测(ii)基于位置向量的高效细粒度用户签到位置预测 3.移动通信安全技术演进与5g安全 系统安全前沿论坛1.云计算安全研究进展及未来趋势(1)云系统的可信构建（用户密码安全、执行环境安全、sgx硬件安全防护）(i)可信执行环境【动态可信环境构建、抵御回滚攻击】可信云签名系统（用户和虚拟机之间进行安全操作）(ii)云用户安全(iii)函数运行时安全【sgx侧信道攻击】 (2)云服务安全共享（底层虚拟化，分析多层次虚拟化机理）：镜像在离线状态下升级（华科）、镜像动态升级、镜像回滚 (3)云数据安全可控 (4)发展前景：平台容器化、服务边缘化、数据中心sdn化容器化：共享化越高，隔离性弱 2.茶歇 下午下午听了优秀青年学生论坛。中午没时间休息，下午比较困，且坐在后排，拍照的效果不好，所以拍的较少。 7.《Apply for your ph.D and Live Overseas》 马思奇（澳大利亚政府研究所）(1)(2)(3)(4)人头挡住的部分为： Applicable to data patch 上面为修复一个漏洞的方法，下面为修复很多漏洞的模型(5)(6)(7)(8)(9)(10) 笔记：她的研究方向为漏洞挖掘和修复，主要是修复。 8.《Hacking for Fun》 初佳奇（360信息安全部）(1)(2) 笔记：(i)故意使电脑中毒，然后查资料恢复电脑。(ii)后期，该同学开始做SRC。(iii)工作接触的领域可能与喜欢或擅长的领域不同，考验学习新知识的能力。(iv)做技术要低调，不要show off。(v)要经得住诱惑，守法。 9.《用十二年做一线安全研究》 李卷孺（上海交通大学）(1)(2)(3)(4) 笔记：(i)14年他和同学成立了0ops。(ii)通过学术演讲宣传自我，后面有些工业界的人来合作。(iii)个人研究困难，团队合作才好，团队成员互相帮助。(iv)提升学术分享和表达能力，比如博客、微信公众号、知乎专栏、微博。(v)不要定义边界：要做什么，不要做什么。有的没做过的事可以尝试去做。(vi)自省。 10.《读博，从入门到延毕》 杨文博（上海交通大学）(1)(2)(3)(4)(5) 笔记：他倾向于科研，因为工作大多只是针对在某个局限的领域，有约束，而科研更自由，想搞的方向更自由 11.《网络安全研究拾趣》 郑晓峰（360企业安全技术研究院）(1)(2)(3)(4)(5)(6)(7) 笔记：在他举例的https网页的google邮件案例中，如果结合社工（比如伪装邮件联系人），效果更好。他还举了一个关于劫持订单（京东）的例子。 12.《Mobile Security,From 0 To 1》 张磊（复旦大学）(1)(2)(3)(4)(5) 笔记：（来源于现场提问环节）Q:如何提供科研idea的来源？A:平时可以关注freebuf、玄武公众号、四大安全顶会。 总结主要有以下下几点：(i)希望以后加强自律，养成早睡早起等等习惯。(ii)个人研究困难，团队合作才好，团队成员互相帮助。(iii)提升学术分享和表达能力，比如博客、微信公众号、知乎专栏、微博。(iv)不要定义边界：要做什么，不要做什么。有的没做过的事可以尝试去做。","categories":[{"name":"Academic Forum","slug":"Academic-Forum","permalink":"https://hungryfoolou.github.io/categories/Academic-Forum/"}],"tags":[]},{"title":"七牛云图片测试","slug":"七牛云图片测试","date":"2018-11-24T12:19:52.000Z","updated":"2018-12-17T01:56:10.169Z","comments":true,"path":"2018/11/24/七牛云图片测试/","link":"","permalink":"https://hungryfoolou.github.io/2018/11/24/七牛云图片测试/","excerpt":"","text":"更新(2018/12/17)已放弃七牛云，上个月莫名其妙花了我几毛钱，这个月又说之前使用的域名即将被收回，只得买了域名，可我舍不得，用github多爽 原文：想把鼠标格式改为“滑稽”表情包，把图片上传到七牛云，下面是测试结果。emoji：然而，按照知乎的教程一顿操作之后，鼠标格式还是没变，嘤嘤嘤","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"pycharm向已有的项目中导入新文件","slug":"pycharm向已有的项目中导入新文件","date":"2018-10-22T14:18:48.000Z","updated":"2018-10-22T14:19:24.796Z","comments":true,"path":"2018/10/22/pycharm向已有的项目中导入新文件/","link":"","permalink":"https://hungryfoolou.github.io/2018/10/22/pycharm向已有的项目中导入新文件/","excerpt":"","text":"Question Description用git clone从github下载项目A并用pycharm打开后，向A中加入文件b，pycharm提示b中的很多的”import”是错误的，然而实际可以运行。 Solutionpycharm不会将当前文件目录自动加入自己的sourse path，对提示错误的文件b右键，make directory as–&gt;sources path。将当前工作的文件夹加入source_path。参考 知乎的回答 。","categories":[{"name":"Python","slug":"Python","permalink":"https://hungryfoolou.github.io/categories/Python/"}],"tags":[]},{"title":"The first day of XIDIAN University","slug":"The-first-day-of-XIDIAN-University","date":"2018-09-02T12:27:47.000Z","updated":"2018-09-02T12:56:48.789Z","comments":true,"path":"2018/09/02/The-first-day-of-XIDIAN-University/","link":"","permalink":"https://hungryfoolou.github.io/2018/09/02/The-first-day-of-XIDIAN-University/","excerpt":"","text":"I completed the opening of the journal in the morning.At afernoon,I got a fitness card for a year in the old complex building with the newer gym.Here I come,XIDIAN!","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"How to insert a video into blog","slug":"how-to-upload-a-video-into-blog","date":"2018-08-23T08:11:31.000Z","updated":"2018-08-23T12:02:08.965Z","comments":true,"path":"2018/08/23/how-to-upload-a-video-into-blog/","link":"","permalink":"https://hungryfoolou.github.io/2018/08/23/how-to-upload-a-video-into-blog/","excerpt":"","text":"I found tow methods to insert a video into blog. FirstOne method is to use dplayer github. For example1&#123;% dplayer &quot;url=http://olfa3o6q1.bkt.clouddn.com/%5B4K-60FPS%5D%20Avengers%20-%20Infinity%20War%20%7C%20Official%20Trailer%20%7C%202018.mp4&quot; &quot;api=https://api.prprpr.me/dplayer/&quot; &quot;id=&quot; &quot;loop=false&quot; %&#125; (function(){var player = new DPlayer({\"container\":document.getElementById(\"dplayer0\"),\"video\":{\"url\":\"http://olfa3o6q1.bkt.clouddn.com/%5B4K-60FPS%5D%20Avengers%20-%20Infinity%20War%20%7C%20Official%20Trailer%20%7C%202018.mp4\"},\"danmaku\":{\"api\":\"https://api.prprpr.me/dplayer/\"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() SecondAnother method is to use hexo-tag-youku github. For example123&#123;% youku height=50% width=100% %&#125;XMTI3MDY1MjUy&#123;% endyouku %&#125; XMTI3MDY1MjUy is the id of one video of youku.You can get it from the url. If you use chrome,please allow chrome run the scripts unsafe in order to watch video.（如果使用chrome等浏览器，可能会拦截视频脚本，允许加载才可观看视频）","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"Add background music of the blog","slug":"Add-background-music-of-the-blog","date":"2018-08-23T04:58:10.000Z","updated":"2018-08-23T12:02:10.144Z","comments":true,"path":"2018/08/23/Add-background-music-of-the-blog/","link":"","permalink":"https://hungryfoolou.github.io/2018/08/23/Add-background-music-of-the-blog/","excerpt":"","text":"I like music,so I add background music into my blog.The music is called 天空之城.You can listen to it on the lefe side of my blog by computer(not smart phone). The video below is Jou Hisaishi’s classical concert of City of the sky. If you use chrome,please allow chrome run the scripts unsafe in order to watch video.（如果使用chrome等浏览器，可能会拦截视频脚本，允许加载才可观看视频）","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"Smart phone changed","slug":"Smart phone changed","date":"2018-08-17T03:11:18.000Z","updated":"2018-12-17T01:47:25.898Z","comments":true,"path":"2018/08/17/Smart phone changed/","link":"","permalink":"https://hungryfoolou.github.io/2018/08/17/Smart phone changed/","excerpt":"","text":"It is the forth time that my honor six had problems with the old battery.I have used the honor six for three years nearly.Lack of money,I buy a cheap smart phone,called xiaomi 8se.It seems to work well so far which I got yesterday.The MIUI System is beautiful while there are some ads with it. Desktop’s photo of the xioami8se are listed as follows:","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"Theme changed","slug":"Theme changed","date":"2018-08-16T07:33:14.000Z","updated":"2018-08-23T04:43:07.058Z","comments":true,"path":"2018/08/16/Theme changed/","link":"","permalink":"https://hungryfoolou.github.io/2018/08/16/Theme changed/","excerpt":"","text":"Inspired by a blog of a friend,I changed the theme of my blog.hhhhhh……","categories":[{"name":"Diary","slug":"Diary","permalink":"https://hungryfoolou.github.io/categories/Diary/"}],"tags":[]},{"title":"1013. 数素数 (20)","slug":"1013-数素数-20","date":"2017-04-11T15:33:14.000Z","updated":"2018-08-23T03:26:48.437Z","comments":true,"path":"2017/04/11/1013-数素数-20/","link":"","permalink":"https://hungryfoolou.github.io/2017/04/11/1013-数素数-20/","excerpt":"","text":"题目 题目链接令Pi表示第i个素数。现任给两个正整数M &lt;= N &lt;= 104，请输出PM到PN的所有素数。 输入格式： 输入在一行中给出M和N，其间以空格分隔。 输出格式： 输出从PM到PN的所有素数，每10个数字占1行，其间以空格分隔，但行末不得有多余空格。 输入样例： 5 27 输出样例： 11 13 17 19 23 29 31 37 41 4347 53 59 61 67 71 73 79 83 8997 101 103 思路： 参照：参照 的素数定理公式 x/ln(x) 其中X是数的范围，这个公式能得到在X范围内大概的素数个数比如说：我要大概10000个素数，那素数分布的范围大概是多少呢？ 根据公式 X/ln(X) 求出X = 120000 时 大概会有 10260个素数，所以这时候我们就设数表的大小为120000 代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;string.h&gt;int const maxn = 120000;int prime[maxn];void init()&#123; memset(prime, 0, sizeof(prime)); prime[0] = prime[1] = 1; for (int i = 2; i &lt; maxn; i++) &#123; if (prime[i] == 0) &#123; for (int j = i + i; j &lt; maxn; j += i)&#123; prime[j] = 1; &#125; &#125; &#125;&#125;int main()&#123; init(); int m, n; scanf(\"%d%d\",&amp;m,&amp;n); int count = 0,number=0; for (int i = 2; i &lt; maxn; i++) &#123; if (prime[i] == 0) &#123; count++; if (count &gt;= m&amp;&amp;count &lt;= n) &#123; number++; printf(\"%d\",i); if (number % 10 == 0)printf(\"\\n\"); else if(count!=n)printf(\" \"); else if(count==n)break; &#125; &#125; &#125; return 0;&#125;","categories":[{"name":"ACM","slug":"ACM","permalink":"https://hungryfoolou.github.io/categories/ACM/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2017-03-27T10:58:40.912Z","updated":"2017-03-27T10:58:40.912Z","comments":true,"path":"2017/03/27/hello-world/","link":"","permalink":"https://hungryfoolou.github.io/2017/03/27/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}